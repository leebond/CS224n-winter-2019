{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "from utils.utils import normalizeRows, softmax\n",
    "\n",
    "try:\n",
    "    import icecream as ic\n",
    "except ModuleNotFoundError:\n",
    "    print('need to install icecream package \"\"!pip install icecream\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy (Naive Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
    "                      (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow. \n",
    "\n",
    "    # similarity metric of outside word vectors (matrix) with center word vector\n",
    "    # one for each word \n",
    "    x = np.dot(outsideVectors, centerWordVec) # outsideVectors:(V,D) X centerWordVec(D,) = (V,)\n",
    "\n",
    "    # softmax transforms similarity metric into (loss) likelihood\n",
    "    y_hat = softmax(x) # (V,)\n",
    "\n",
    "    # a mathematical tradition to apply -log transformation to exp function ie. cross entropy\n",
    "    j = -np.log(y_hat) # (V,)\n",
    "\n",
    "    # cross entropy loss of outside word\n",
    "    loss = j[outsideWordIdx] # (1,)\n",
    "\n",
    "    # we have derived the derivative of the gradient of loss function wrt to centerWordVec\n",
    "    y_hat[outsideWordIdx] = y_hat[outsideWordIdx] - 1 # (V,)\n",
    "    gradCenterVec = np.dot(outsideVectors.T, y_hat) # outsideVectors:(D,V) X y_hat:(V,) gradient of embeddings of center word should change with cross entropy loss\n",
    "    \n",
    "    # we have derived the derivative of the gradient of loss function wrt to outsideVectors\n",
    "    gradOutsideVecs = np.outer(y_hat, centerWordVec) # gradient of embeddings of outside words should change with cross entropy loss as well\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, \n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) for all words in vocab\n",
    "                        (V in pdf handout)\n",
    "    outsideVectors -- outside word vectors (as rows) for all words in vocab\n",
    "                    (U in pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVecs -- the gradient with respect to the center word vectors\n",
    "            (dJ / dV in the pdf handout)\n",
    "    gradOutsideVectors -- the gradient with respect to the outside word vectors\n",
    "                        (dJ / dU in the pdf handout)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    centerWord_idx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[centerWord_idx]\n",
    "    for word in outsideWords:\n",
    "        outside_id = word2Ind[word]\n",
    "        loss_, gradCenter_, gradOutside_ = word2vecLossAndGradient(centerWordVec,outside_id,outsideVectors,dataset)\n",
    "        loss += loss_\n",
    "        gradCenterVecs[centerWord_idx] += gradCenter_\n",
    "        gradOutsideVectors += gradOutside_\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "    dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "Skip-Gram with naiveSoftmaxLossAndGradient\n",
      "Your Result:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      "[[ 0.          0.          0.        ]\n",
      "[ 0.          0.          0.        ]\n",
      "[-1.26947339 -1.36873189  2.45158957]\n",
      "[ 0.          0.          0.        ]\n",
      "[ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      "[[-0.41045956  0.18834851  1.43272264]\n",
      "[ 0.38202831 -0.17530219 -1.33348241]\n",
      "[ 0.07009355 -0.03216399 -0.24466386]\n",
      "[ 0.09472154 -0.04346509 -0.33062865]\n",
      "[-0.13638384  0.06258276  0.47605228]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Results ===\")\n",
    "\n",
    "print (\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "print (\"Your Result:\")\n",
    "print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "            dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset) \n",
    "    )\n",
    ")\n",
    "\n",
    "print (\"Expected Result: Value should approximate these:\")\n",
    "print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    "[[ 0.          0.          0.        ]\n",
    "[ 0.          0.          0.        ]\n",
    "[-1.26947339 -1.36873189  2.45158957]\n",
    "[ 0.          0.          0.        ]\n",
    "[ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    "[[-0.41045956  0.18834851  1.43272264]\n",
    "[ 0.38202831 -0.17530219 -1.33348241]\n",
    "[ 0.07009355 -0.03216399 -0.24466386]\n",
    "[ 0.09472154 -0.04346509 -0.33062865]\n",
    "[-0.13638384  0.06258276  0.47605228]]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy (negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-95-ad261de89ed5>, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-95-ad261de89ed5>\"\u001b[1;36m, line \u001b[1;32m85\u001b[0m\n\u001b[1;33m    return loss, gradCenterVec.reshape(-1,1), gradOutsideVecs\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "\n",
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # similarity metric of outside word vectors (matrix) with center word vector\n",
    "    # one for each word \n",
    "    x = np.dot(outsideVectors, centerWordVec) # outsideVectors:(V,D) X centerWordVec(D,) = (V,)\n",
    "\n",
    "    # softmax transforms similarity metric into (loss) likelihood\n",
    "    y_hat = softmax(x) # (V,)\n",
    "    \n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "#     y_hat_k = sigmoid(x)\n",
    "\n",
    "    # a mathematical tradition to apply -log transformation to exp function ie. cross entropy\n",
    "    j = -np.log(y_hat) # (V,)\n",
    "#     j_k = np.log(y_hat_k)\n",
    "\n",
    "    # cross entropy loss of outside word\n",
    "    loss = j[outsideWordIdx] - np.sum(-j[negSampleWordIndices]) # (1,)\n",
    "#     print(loss)\n",
    "\n",
    "    # we have derived the derivative of the gradient of loss function wrt to centerWordVec\n",
    "#     y_hat[outsideWordIdx] = y_hat[outsideWordIdx] - 1 # (V,)\n",
    "#     y_hat[negSampleWordIndices] = y_hat[negSampleWordIndices] - 1\n",
    "    y_hat_CenterVec = y_hat[outsideWordIdx]\n",
    "#     y_hat[indices] -= 1\n",
    "    print(y_hat)\n",
    "\n",
    "    gradCenterVec = np.dot(outsideVectors.T, y_hat_CenterVec) # outsideVectors:(D,V) X y_hat:(V,) gradient of embeddings of center word should change with cross entropy loss\n",
    "    print(gradCenterVec.shape)\n",
    "    # we have derived the derivative of the gradient of loss function wrt to outsideVectors\n",
    "    y_hat_OutsideVec = y_hat\n",
    "    print(y_hat_OutsideVec.reshape(-1,1).shape)\n",
    "#     gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "    gradOutsideVecs = np.outer(y_hat_OutsideVec.reshape(-1,1), centerWordVec.T) # gradient of embeddings of outside words should change with cross entropy loss as well\n",
    "    index_count = np.bincount(negSampleWordIndices)\n",
    "    for i in np.unique(negSampleWordIndices):\n",
    "        gradOutsideVecs[i] *= index_count[i]\n",
    "    \n",
    "#     print(gradOutsideVecs)\n",
    "    print(gradOutsideVecs.shape)\n",
    "'''\n",
    "(5, 3)\n",
    "(5, 1)\n",
    "(5, 3)\n",
    "'''\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec.reshape(-1,1), gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "[0.30381628 0.09182426 0.20107162 0.16317675 0.2401111 ]\n",
      "(5, 3)\n",
      "(5, 1)\n",
      "(5, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (3,) doesn't match the broadcast shape (5,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-6536bf8bca7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mword2vecModel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2Ind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     word2vecLossAndGradient=negSamplingLossAndGradient),\n\u001b[1;32m---> 50\u001b[1;33m     dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n\u001b[0m",
      "\u001b[1;32mE:\\Geekout\\CS224n-winter-2019\\a2\\utils\\gradcheck.py\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[1;34m(f, x, gradientText)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mrndstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrndstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mfx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Evaluate function value at original point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-4\u001b[0m        \u001b[1;31m# Do not change this!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-6536bf8bca7d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(vec)\u001b[0m\n\u001b[0;32m     47\u001b[0m gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[0;32m     48\u001b[0m     \u001b[0mword2vecModel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2Ind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     word2vecLossAndGradient=negSamplingLossAndGradient),\n\u001b[0m\u001b[0;32m     50\u001b[0m     dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
      "\u001b[1;32m<ipython-input-5-01195215a285>\u001b[0m in \u001b[0;36mword2vec_sgd_wrapper\u001b[1;34m(word2vecModel, word2Ind, wordVectors, dataset, windowSize, word2vecLossAndGradient)\u001b[0m\n\u001b[0;32m     14\u001b[0m         c, gin, gout = word2vecModel(\n\u001b[0;32m     15\u001b[0m             \u001b[0mcenterWord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2Ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenterWordVectors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0moutsideVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vecLossAndGradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         )\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-96945e81f847>\u001b[0m in \u001b[0;36mskipgram\u001b[1;34m(currentCenterWord, windowSize, outsideWords, word2Ind, centerWordVectors, outsideVectors, dataset, word2vecLossAndGradient)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mloss_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradCenter_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradOutside_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vecLossAndGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenterWordVec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutside_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutsideVectors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mgradCenterVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcenterWord_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgradCenter_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mgradOutsideVectors\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgradOutside_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m### END YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (3,) doesn't match the broadcast shape (5,3)"
     ]
    }
   ],
   "source": [
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "'''\n",
    "skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "\n",
    "word2vec_sgd_wrapper(\n",
    "    word2vecModel=skipgram, word2Ind=dummy_tokens, wordVectors=vec, dataset=dataset, windowSize=5,\\\n",
    "    word2vecLossAndGradient=negSamplingLossAndGradient)\n",
    "\n",
    "centerWordVectors = wordVectors[:int(N/2),:]\n",
    "outsideVectors = wordVectors[int(N/2):,:]\n",
    "centerWord_idx = word2Ind[currentCenterWord]\n",
    "centerWordVec = centerWordVectors[centerWord_idx]\n",
    "for word in outsideWords:\n",
    "    outside_id = word2Ind[word]\n",
    "    \n",
    "word2vecLossAndGradient(centerWordVec,outside_id,outsideVectors,dataset)\n",
    "negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "'''\n",
    "print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    word2vecModel=skipgram, word2Ind=dummy_tokens, wordVectors=vec, dataset=dataset, windowSize=5,\\\n",
    "    word2vecLossAndGradient=naiveSoftmaxLossAndGradient),\n",
    "    dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "    word2vecModel=skipgram, word2Ind=dummy_tokens, wordVectors=vec, dataset=dataset, windowSize=5,\\\n",
    "    word2vecLossAndGradient=negSamplingLossAndGradient),\n",
    "    dummy_vectors, \"negSamplingLossAndGradient Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram with negSamplingLossAndGradient\n",
      "Your Result:\n",
      "Loss: -44.22955455173793\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.52204914 -1.04083814  0.29121161]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.05485323  0.02517062  0.191467  ]\n",
      " [-0.05485323  0.02517062  0.191467  ]\n",
      " [-0.05485323  0.02517062  0.191467  ]\n",
      " [-0.05485323  0.02517062  0.191467  ]\n",
      " [-0.05485323  0.02517062  0.191467  ]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      "[[ 0.          0.          0.        ]\n",
      "[ 0.          0.          0.        ]\n",
      "[-4.54650789 -1.85942252  0.76397441]\n",
      "[ 0.          0.          0.        ]\n",
      "[ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      "[[-0.69148188  0.31730185  2.41364029]\n",
      "[-0.22716495  0.10423969  0.79292674]\n",
      "[-0.45528438  0.20891737  1.58918512]\n",
      "[-0.31602611  0.14501561  1.10309954]\n",
      "[-0.80620296  0.36994417  2.81407799]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Skip-Gram with negSamplingLossAndGradient\")   \n",
    "print (\"Your Result:\")\n",
    "'''\n",
    "skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, \n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient)\n",
    "word2vecLossAndGradient(centerWordVec,outside_id,outsideVectors,dataset)\n",
    "negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "'''\n",
    "print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "    *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
    "        dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "    )\n",
    ")\n",
    "print (\"Expected Result: Value should approximate these:\")\n",
    "print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    "[[ 0.          0.          0.        ]\n",
    "[ 0.          0.          0.        ]\n",
    "[-4.54650789 -1.85942252  0.76397441]\n",
    "[ 0.          0.          0.        ]\n",
    "[ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    "[[-0.69148188  0.31730185  2.41364029]\n",
    "[-0.22716495  0.10423969  0.79292674]\n",
    "[-0.45528438  0.20891737  1.58918512]\n",
    "[-0.31602611  0.14501561  1.10309954]\n",
    "[-0.80620296  0.36994417  2.81407799]]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "\n",
    "    print (\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "    print (\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "            *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset) \n",
    "        )\n",
    "    )\n",
    "\n",
    "    print (\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-1.26947339 -1.36873189  2.45158957]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.41045956  0.18834851  1.43272264]\n",
    " [ 0.38202831 -0.17530219 -1.33348241]\n",
    " [ 0.07009355 -0.03216399 -0.24466386]\n",
    " [ 0.09472154 -0.04346509 -0.33062865]\n",
    " [-0.13638384  0.06258276  0.47605228]]\n",
    "    \"\"\")\n",
    "\n",
    "    print (\"Skip-Gram with negSamplingLossAndGradient\")   \n",
    "    print (\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
    "            dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "        )\n",
    "    )\n",
    "    print (\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-4.54650789 -1.85942252  0.76397441]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    " Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.69148188  0.31730185  2.41364029]\n",
    " [-0.22716495  0.10423969  0.79292674]\n",
    " [-0.45528438  0.20891737  1.58918512]\n",
    " [-0.31602611  0.14501561  1.10309954]\n",
    " [-0.80620296  0.36994417  2.81407799]]\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
